{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import util\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: A dedicated Electronics and Telecommunication professional with a strong background in machine learning, web development, and cloud platforms. Proven ability to lead projects, solve complex problems, and continuously learn new technologies to drive innovation and efficiency.', 'Contact: Name: Aneesh Patne, Email: aneeshpatne12@gmail.com, Linkedin: https://www.linkedin.com/in/aneeshpatne, Github: https://github.com/aneeshpatne, Leetcode: https://leetcode.com/aneeshpatne', 'Education:\\nDegree: M.Tech in Electronics and Telecommunication\\nInstitution: Veermata Jijabai Technological Institute\\nLocation: Mumbai, Maharashtra\\nDuration: 2023 - 2025\\nDetails: Specialized in Machine Learning and Signal Processing. Relevant coursework includes Advanced Algorithms, Neural Networks, and Communication Systems.', 'Education:\\nDegree: B.Tech in Electronics and Telecommunication\\nInstitution: Thakur College of Engineering and Technology\\nLocation: Mumbai, Maharashtra\\nDuration: 2019 - 2023\\nDetails: Graduated with First Class Honors. Engaged in multiple projects focusing on embedded systems and wireless communication.', \"Skills: Technical_Skills: ['Python', 'Machine Learning', 'React', 'Node.js', 'TensorFlow', 'MATLAB', 'Express', 'MongoDB', 'Google Vertex AI', 'SvelteKit'], Soft_Skills: ['Problem-Solving', 'Team Leadership', 'Effective Communication', 'Adaptability', 'Time Management']\", \"Projects:\\nTitle: Socio-Economic Impact of Pollution on Life Expectancy\\nDescription: A machine learning project analyzing the impact of air pollution on life expectancy, integrating multiple socio-economic parameters.\\nTechnologies Used: ['Machine Learning', 'Google Vertex AI', 'SvelteKit', 'Express']\\nDuration: January 2024 - Present\\nTags: ['Machine Learning', 'Data Analysis', 'Sustainability']\", \"Projects:\\nTitle: Personal Chatbot\\nDescription: Developed a chatbot to interactively present my resume, utilizing Retrieval-Augmented Generation (RAG) for accurate and context-specific responses.\\nTechnologies Used: ['Python', 'GPT-3', 'Flask', 'FAISS']\\nDuration: March 2023 - June 2023\\nTags: ['Chatbot', 'al Language Processing', 'AI', 'Web Development']\", \"Projects:\\nTitle: IoT and ML based Weather Prediction System\\nDescription: Designed and implemented an embedded system for automating household tasks, enhancing efficiency and reliability.\\nTechnologies Used: ['C++', 'Arduino', 'IoT']\\nDuration: September 2022 - December 2022\\nTags: ['Embedded Systems', 'IoT', 'Automation']\", 'Certifications:\\nTitle: Machine Learning Specialist\\nIssuing Organization: Coursera\\nDate Awarded: June 2023\\nCredential Id: ML12345', 'Certifications:\\nTitle: Full Stack Web Development\\nIssuing Organization: Udemy\\nDate Awarded: April 2022\\nCredential Id: FSWD67890']\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as file:\n",
    "    resume = json.load(file)\n",
    "def create_documents_dynamic(resume):\n",
    "    documents = []\n",
    "\n",
    "    for key, value in resume.items():\n",
    "        if isinstance(value, dict):  # Handle nested dictionary (e.g., contact information)\n",
    "            section = f\"{key.replace('_', ' ').title()}: \" + \", \".join(\n",
    "                f\"{sub_key.title()}: {sub_value}\" for sub_key, sub_value in value.items()\n",
    "            )\n",
    "            documents.append(section)\n",
    "\n",
    "        elif isinstance(value, list):  # Handle lists (e.g., education, projects)\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):  # Handle list of dictionaries\n",
    "                    section = f\"{key.replace('_', ' ').title()}:\\n\" + \"\\n\".join(\n",
    "                        f\"{sub_key.replace('_', ' ').title()}: {sub_value}\" for sub_key, sub_value in item.items()\n",
    "                    )\n",
    "                    documents.append(section)\n",
    "                else:  # Handle plain list\n",
    "                    documents.append(f\"{key.replace('_', ' ').title()}: {', '.join(value)}\")\n",
    "        \n",
    "        elif isinstance(value, str):  # Handle plain string fields\n",
    "            documents.append(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        else:  # Handle unexpected formats\n",
    "            documents.append(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example Usage\n",
    "documents = create_documents_dynamic(resume)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00,  2.28it/s]\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = embedding_model.encode(documents, convert_to_tensor=False, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = np.array(document_embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = faiss.StandardGpuResources()\n",
    "index_flat = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index.add(document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])  # Using L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'resume_index.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.txt', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(doc.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, top_k=3):\n",
    "\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    \n",
    "\n",
    "    index = faiss.read_index('resume_index.faiss')\n",
    "    \n",
    "  \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "\n",
    "    with open('documents.txt', 'r') as f:\n",
    "        all_documents = f.readlines()\n",
    "    \n",
    "\n",
    "    relevant_docs = [all_documents[idx].strip() for idx in indices[0]]\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"D:/Llama-3.2-1B-Instruct\"  # Replace with your actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    ").to(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Local LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projects: Title: Socio-Economic Impact of Pollution on Life Expectancy Description: A machine learning project analyzing the impact of air pollution on life expectancy, integrating multiple socio-economic parameters. Technologies Used: ['Machine Learning', 'Google Vertex AI', 'SvelteKit', 'Express'] Duration: January 2024 - Present Tags: ['Machine Learning', 'Data Analysis', 'Sustainability']\n",
      "Projects: Title: IoT and ML based Weather Prediction System Description: Designed and implemented an embedded system for automating household tasks, enhancing efficiency and reliability. Technologies Used: ['C++', 'Arduino', 'IoT'] Duration: September 2022 - December 2022 Tags: ['Embedded Systems', 'IoT', 'Automation']\n",
      "Contact: Name: Aneesh Patne, Email: aneeshpatne12@gmail.com, Linkedin: https://www.linkedin.com/in/aneeshpatne, Github: https://github.com/aneeshpatne, Leetcode: https://leetcode.com/aneeshpatne\n",
      "Unfortunately, I'm sorry, this information is not available.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query):\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query)\n",
    "    \n",
    "    # Concatenate the documents\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    print(context)\n",
    "    # Create the prompt\n",
    "    prompt = (\n",
    "    \"You are an AI assistant answering questions about a person's resume. \"\n",
    "    \"Only use the information provided in the context below to answer the question. \"\n",
    "    \"If the answer cannot be found in the context, respond with 'I'm sorry, this information is not available.'\\n\\n\"\n",
    "    f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    ")\n",
    "\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer.encode(prompt, return_tensors='pt')\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(inputs, max_length=500, do_sample=True, temperature=0.2)\n",
    "    \n",
    "    # Decode the response\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer part\n",
    "    answer = answer.split('Answer:')[-1].strip()\n",
    "    return answer\n",
    "query = \"Explain me about the project Socio-Economic Impact of Pollution on Life Expectancy\"\n",
    "print(generate_response(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_similarity_per_segment(query, documents):\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    max_similarity = 0\n",
    "    best_doc = None\n",
    "    for doc in documents:\n",
    "        doc_embedding = embedding_model.encode(doc, convert_to_tensor=True)\n",
    "        similarity = util.cos_sim(query_embedding, doc_embedding).item()\n",
    "        if similarity > max_similarity:\n",
    "            max_similarity = similarity\n",
    "            best_doc = doc\n",
    "    \n",
    "    return max_similarity, best_doc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cloud LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved documents Found\n",
      "Calling OpenRouter API\n",
      "Yes, Aneesh can use Python as it is listed under both the 'Technical_Skills' and 'Projects' sections of the provided context.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "# Initialize OpenAI client\n",
    "client = OpenAI(\n",
    "    base_url=\"https://openrouter.ai/api/v1\",\n",
    "    api_key=os.getenv(\"OPEN_ROUTER_API_KEY\"),\n",
    ")\n",
    "\n",
    "def generate_response(query):\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query)\n",
    "    print(\"Retrieved documents Found\")\n",
    "    \n",
    "    if not relevant_docs:\n",
    "        return \"I'm sorry, this information is not available in the resume.\"\n",
    "    \n",
    "    # Compute similarity per segment\n",
    "    similarity, best_doc = compute_similarity_per_segment(query, relevant_docs)\n",
    "\n",
    "    # Use the most relevant document as context\n",
    "    context = \"\\n\".join(relevant_docs)  # Combine documents into a single string\n",
    "    prompt = (\n",
    "        \"You are 'Chotu', Aneesh's assistant, answering questions about a person's resume. \"\n",
    "        \"Use only the provided context to answer the question. \"\n",
    "        \"Do not generate any information that is not explicitly stated in the context. \"\n",
    "        \"Dont Explicitly Mention the name of the Context. \"\n",
    "        \"If the information is not available, respond with: 'I'm sorry, this information is not available.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Call the OpenRouter API\n",
    "        print(\"Calling OpenRouter API\")\n",
    "        completion = client.chat.completions.create(\n",
    "            model=\"mistralai/mistral-7b-instruct:free\",  # You can use \"gpt-4\" or other models supported on OpenRouter.\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": (\n",
    "                        \"You are a helpful assistant that answers questions about resumes based \"\n",
    "                        \"only on the provided context.\"\n",
    "                    )\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": prompt\n",
    "                }\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # Extract the response content\n",
    "        answer = completion.choices[0].message.content.strip()\n",
    "        return answer\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return \"An error occurred while processing your request.\"\n",
    "\n",
    "# Example query\n",
    "query = \"Can Aneesh python ?\"\n",
    "print(generate_response(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
