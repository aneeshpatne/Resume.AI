{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import numpy as np\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sentence_transformers import util\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Summary: A dedicated Electronics and Telecommunication professional with a strong background in machine learning, web development, and cloud platforms. Proven ability to lead projects, solve complex problems, and continuously learn new technologies to drive innovation and efficiency.', 'Contact: Name: Aneesh Patne, Email: aneeshpatne12@gmail.com, Linkedin: https://www.linkedin.com/in/aneeshpatne, Github: https://github.com/aneeshpatne, Leetcode: https://leetcode.com/aneeshpatne', 'Education:\\nDegree: M.Tech in Electronics and Telecommunication\\nInstitution: Veermata Jijabai Technological Institute\\nLocation: Mumbai, Maharashtra\\nDuration: 2023 - 2025\\nDetails: Specialized in Machine Learning and Signal Processing. Relevant coursework includes Advanced Algorithms, Neural Networks, and Communication Systems.', 'Education:\\nDegree: B.Tech in Electronics and Telecommunication\\nInstitution: Thakur College of Engineering and Technology\\nLocation: Mumbai, Maharashtra\\nDuration: 2019 - 2023\\nDetails: Graduated with First Class Honors. Engaged in multiple projects focusing on embedded systems and wireless communication.', \"Experience:\\nTitle: Software Engineer Intern\\nCompany: Tech Innovators Pvt. Ltd.\\nLocation: Mumbai, Maharashtra\\nDuration: June 2022 - August 2022\\nResponsibilities: ['Developed scalable web applications using React and Node.js.', 'Collaborated with cross-functional teams to define project requirements and deliverables.', 'Implemented RESTful APIs to enhance application functionality and performance.']\\nTechnologies Used: ['React', 'Node.js', 'Express', 'MongoDB']\", \"Experience:\\nTitle: Research Assistant\\nCompany: AI Research Lab, VJTI\\nLocation: Mumbai, Maharashtra\\nDuration: September 2023 - Present\\nResponsibilities: ['Conducted research on machine learning algorithms for signal processing.', 'Developed and fine-tuned models to improve data accuracy and processing speed.', 'Published findings in reputable journals and presented at conferences.']\\nTechnologies Used: ['Python', 'TensorFlow', 'MATLAB', 'LaTeX']\", \"Skills: Technical_Skills: ['Python', 'Machine Learning', 'React', 'Node.js', 'TensorFlow', 'MATLAB', 'Express', 'MongoDB', 'Google Vertex AI', 'SvelteKit'], Soft_Skills: ['Problem-Solving', 'Team Leadership', 'Effective Communication', 'Adaptability', 'Time Management']\", \"Projects:\\nTitle: Socio-Economic Impact of Pollution on Life Expectancy\\nDescription: A machine learning project analyzing the impact of air pollution on life expectancy, integrating multiple socio-economic parameters.\\nTechnologies Used: ['Machine Learning', 'Google Vertex AI', 'SvelteKit', 'Express']\\nDuration: January 2024 - Present\\nTags: ['Machine Learning', 'Data Analysis', 'Sustainability']\", \"Projects:\\nTitle: Personal Chatbot\\nDescription: Developed a chatbot to interactively present my resume, utilizing Retrieval-Augmented Generation (RAG) for accurate and context-specific responses.\\nTechnologies Used: ['Python', 'GPT-3', 'Flask', 'FAISS']\\nDuration: March 2023 - June 2023\\nTags: ['Chatbot', 'Natural Language Processing', 'AI', 'Web Development']\", \"Projects:\\nTitle: Embedded Systems Automation\\nDescription: Designed and implemented an embedded system for automating household tasks, enhancing efficiency and reliability.\\nTechnologies Used: ['C++', 'Arduino', 'IoT']\\nDuration: September 2022 - December 2022\\nTags: ['Embedded Systems', 'IoT', 'Automation']\", 'Certifications:\\nTitle: Machine Learning Specialist\\nIssuing Organization: Coursera\\nDate Awarded: June 2023\\nCredential Id: ML12345', 'Certifications:\\nTitle: Full Stack Web Development\\nIssuing Organization: Udemy\\nDate Awarded: April 2022\\nCredential Id: FSWD67890']\n"
     ]
    }
   ],
   "source": [
    "with open('data.json', 'r') as file:\n",
    "    resume = json.load(file)\n",
    "def create_documents_dynamic(resume):\n",
    "    documents = []\n",
    "\n",
    "    for key, value in resume.items():\n",
    "        if isinstance(value, dict):  # Handle nested dictionary (e.g., contact information)\n",
    "            section = f\"{key.replace('_', ' ').title()}: \" + \", \".join(\n",
    "                f\"{sub_key.title()}: {sub_value}\" for sub_key, sub_value in value.items()\n",
    "            )\n",
    "            documents.append(section)\n",
    "\n",
    "        elif isinstance(value, list):  # Handle lists (e.g., education, projects)\n",
    "            for item in value:\n",
    "                if isinstance(item, dict):  # Handle list of dictionaries\n",
    "                    section = f\"{key.replace('_', ' ').title()}:\\n\" + \"\\n\".join(\n",
    "                        f\"{sub_key.replace('_', ' ').title()}: {sub_value}\" for sub_key, sub_value in item.items()\n",
    "                    )\n",
    "                    documents.append(section)\n",
    "                else:  # Handle plain list\n",
    "                    documents.append(f\"{key.replace('_', ' ').title()}: {', '.join(value)}\")\n",
    "        \n",
    "        elif isinstance(value, str):  # Handle plain string fields\n",
    "            documents.append(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "        \n",
    "        else:  # Handle unexpected formats\n",
    "            documents.append(f\"{key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "    return documents\n",
    "\n",
    "# Example Usage\n",
    "documents = create_documents_dynamic(resume)\n",
    "print(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 1/1 [00:00<00:00, 12.92it/s]\n"
     ]
    }
   ],
   "source": [
    "document_embeddings = embedding_model.encode(documents, convert_to_tensor=False, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_embeddings = np.array(document_embeddings).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = faiss.StandardGpuResources()\n",
    "index_flat = faiss.IndexFlatL2(document_embeddings.shape[1])\n",
    "gpu_index = faiss.index_cpu_to_gpu(res, 0, index_flat)\n",
    "gpu_index.add(document_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = faiss.IndexFlatL2(document_embeddings.shape[1])  # Using L2 distance\n",
    "index.add(document_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "faiss.write_index(index, 'resume_index.faiss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('documents.txt', 'w') as f:\n",
    "    for doc in documents:\n",
    "        f.write(doc.replace('\\n', ' ') + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_documents(query, top_k=3):\n",
    "\n",
    "    query_embedding = embedding_model.encode([query], convert_to_tensor=False)\n",
    "    query_embedding = np.array(query_embedding).astype('float32')\n",
    "    \n",
    "\n",
    "    index = faiss.read_index('resume_index.faiss')\n",
    "    \n",
    "  \n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "\n",
    "    with open('documents.txt', 'r') as f:\n",
    "        all_documents = f.readlines()\n",
    "    \n",
    "\n",
    "    relevant_docs = [all_documents[idx].strip() for idx in indices[0]]\n",
    "    return relevant_docs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"D:/Llama-3.2-1B-Instruct\"  # Replace with your actual path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path,\n",
    ").to(\"cpu\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm sorry, this information is not available in the resume.\n"
     ]
    }
   ],
   "source": [
    "def generate_response(query):\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query)\n",
    "    \n",
    "    if not relevant_docs:  # No relevant documents found\n",
    "        return \"I'm sorry, this information is not available in the resume.\"\n",
    "    \n",
    "    # Concatenate the documents\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    \n",
    "    # Compute similarity\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    context_embedding = embedding_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_embedding, context_embedding).item()\n",
    "    \n",
    "    if similarity < 0.3:  # Low similarity threshold\n",
    "        return f\"I'm sorry, this information is not available in the resume. (Similarity: {similarity:.2f})\"\n",
    "    \n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        \"You are an AI assistant answering questions about a person's resume. \"\n",
    "        \"Respond only with information present in the context below. If the answer \"\n",
    "        \"cannot be found, respond with 'I'm sorry, this information is not available in the resume.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer part\n",
    "    answer = answer.split('Answer:')[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example query\n",
    "query = \"Tell me something about the project Socio-Economic Impact of Pollution on Life Expectancy\"\n",
    "print(generate_response(query))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query):\n",
    "    # Retrieve relevant documents\n",
    "    relevant_docs = retrieve_relevant_documents(query)\n",
    "    \n",
    "    if not relevant_docs:  # No relevant documents found\n",
    "        return \"I'm sorry, this information is not available in the resume.\"\n",
    "    \n",
    "    # Concatenate the documents\n",
    "    context = \"\\n\".join(relevant_docs)\n",
    "    \n",
    "    # Compute similarity\n",
    "    query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
    "    context_embedding = embedding_model.encode(context, convert_to_tensor=True)\n",
    "    similarity = util.cos_sim(query_embedding, context_embedding).item()\n",
    "    \n",
    "    if similarity < 0.3:  # Low similarity threshold\n",
    "        return f\"I'm sorry, this information is not available in the resume. (Similarity: {similarity:.2f})\"\n",
    "    \n",
    "    # Construct prompt\n",
    "    prompt = (\n",
    "        \"You are an AI assistant answering questions about a person's resume. \"\n",
    "        \"Respond only with information present in the context below. If the answer \"\n",
    "        \"cannot be found, respond with 'I'm sorry, this information is not available in the resume.'\\n\\n\"\n",
    "        f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "    )\n",
    "    \n",
    "    # Tokenize the input\n",
    "    inputs = tokenizer(prompt, return_tensors='pt', truncation=True)\n",
    "    \n",
    "    # Generate the response\n",
    "    outputs = model.generate(\n",
    "        inputs.input_ids,\n",
    "        max_length=500,\n",
    "        do_sample=True,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    # Decode the response\n",
    "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract the answer part\n",
    "    answer = answer.split('Answer:')[-1].strip()\n",
    "    \n",
    "    return answer\n",
    "\n",
    "# Example query\n",
    "query = \"Tell me something about the project Socio-Economic Impact of Pollution on Life Expectancy\"\n",
    "print(generate_response(query))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
