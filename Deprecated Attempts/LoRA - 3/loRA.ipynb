{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import BitsAndBytesConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model_path = \"D:/Llama-3.2-1B-Instruct\"\n",
    "tokenizer_path = base_model_path \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_path, trust_remote_code=True)\n",
    "if tokenizer.pad_token_id is None:\n",
    "    tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.float16\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True  \n",
    ")\n",
    "model = model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 61 examples [00:00, ? examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['instruction', 'input', 'output'],\n",
      "        num_rows: 61\n",
      "    })\n",
      "})\n",
      "{'instruction': \"What is the candidate's full name?\", 'input': 'Name: Aneesh Patne', 'output': \"The candidate's full name is Aneesh Patne.\"}\n"
     ]
    }
   ],
   "source": [
    "print(dataset)\n",
    "print(dataset['train'][0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    prompts = []\n",
    "    labels = []\n",
    "    for inst, inp, out in zip(examples['instruction'], examples['input'], examples['output']):\n",
    "        # Create the prompt\n",
    "        prompt = f\"Instruction: {inst}\\nInput: {inp}\\nResponse:\"\n",
    "        # Combine prompt and output\n",
    "        combined = f\"{prompt} {out}\"\n",
    "        prompts.append(prompt)\n",
    "        labels.append(out)\n",
    "    return {'prompt': prompts, 'response': labels}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    prompts = [\n",
    "        f\"Instruction: {inst}\\nInput: {inp}\\nResponse:\"\n",
    "        for inst, inp in zip(examples['instruction'], examples['input'])\n",
    "    ]\n",
    "    responses = examples['output']\n",
    "\n",
    "    # Tokenize the combined prompts and responses\n",
    "    encodings = tokenizer(prompts, responses, padding=\"max_length\", truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "\n",
    "    # Initialize labels with the input_ids\n",
    "    labels = encodings.input_ids.clone()\n",
    "\n",
    "    for i in range(len(prompts)):\n",
    "        # Tokenize the prompt to find its length\n",
    "        prompt_encoding = tokenizer(prompts[i], truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        prompt_length = prompt_encoding.input_ids.shape[1]\n",
    "\n",
    "        # Mask the prompt tokens in labels by setting them to -100\n",
    "        labels[i, :prompt_length] = -100\n",
    "\n",
    "    encodings['labels'] = labels\n",
    "\n",
    "    # Convert tensors to lists for the dataset\n",
    "    encodings = {k: v.tolist() for k, v in encodings.items()}\n",
    "\n",
    "    return encodings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 61/61 [00:00<00:00, 1416.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"dataset.jsonl\")\n",
    "\n",
    "# Apply the tokenization function\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"instruction\", \"input\", \"output\"])\n",
    "\n",
    "# Set the format for PyTorch\n",
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([128000,  17077,     25,   3639,  25845,   1587,    279,  16063,   1501,\n",
      "           369,    279,  88252,     85,   2447,   5380,   2566,     25,  88252,\n",
      "            85,     25,   4448,    220,   2366,     19,    482,   5936,    220,\n",
      "          2366,     19,   1234,    279,   5195,  12761,  26323,  71053,    627,\n",
      "          2647,     25,    578,  88252,     85,   2447,   3952,   2035,   1990,\n",
      "          6186,    323,   5936,    220,   2366,     19,    439,    961,    315,\n",
      "           279,   5195,  12761,  26323,     13, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([  -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,   -100,\n",
      "          -100,   -100,    578,  88252,     85,   2447,   3952,   2035,   1990,\n",
      "          6186,    323,   5936,    220,   2366,     19,    439,    961,    315,\n",
      "           279,   5195,  12761,  26323,     13, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009,\n",
      "        128009, 128009, 128009, 128009, 128009, 128009, 128009, 128009])}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(train_dataset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([512])\n",
      "torch.Size([512])\n",
      "torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "sample = tokenized_dataset['train'][0]\n",
    "print(sample['input_ids'].shape)      # Should be torch.Size([512])\n",
    "print(sample['attention_mask'].shape) # Should be torch.Size([512])\n",
    "print(sample['labels'].shape)        # Should be torch.Size([512])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataset\n",
    "if len(tokenized_dataset['train']) > 1:\n",
    "    split_dataset = tokenized_dataset['train'].train_test_split(test_size=0.1, seed=42)\n",
    "    train_dataset = split_dataset['train']\n",
    "    eval_dataset = split_dataset['test']\n",
    "else:\n",
    "    train_dataset = tokenized_dataset['train']\n",
    "    eval_dataset = tokenized_dataset['train']  # Not ideal, but acceptable for very small datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,703,936 || all params: 1,237,518,336 || trainable%: 0.1377\n"
     ]
    }
   ],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],  # Adjust based on model architecture\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anees\\Desktop\\Coding\\llmrepo\\train-LLAMA\\.venv\\Lib\\site-packages\\transformers\\training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llama_finetuned_resume\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    learning_rate=3e-4,\n",
    "    num_train_epochs=10,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=10,\n",
    "    fp16=True,  \n",
    "    save_total_limit=2,\n",
    "    push_to_hub=False,  \n",
    "    report_to=\"wandb\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\anees\\_netrc\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "wandb.login()  # You'll be prompted to enter your W&B API key\n",
    "\n",
    "training_args.report_to = \"wandb\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anees\\AppData\\Local\\Temp\\ipykernel_1420\\4113902299.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,  \n",
    "    eval_dataset=eval_dataset,    \n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [09:49<?, ?it/s]\n",
      " 13%|â–ˆâ–Ž        | 4/30 [00:08<00:51,  1.97s/it]\n",
      " 13%|â–ˆâ–Ž        | 4/30 [00:09<00:51,  1.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.3368794918060303, 'eval_runtime': 0.4997, 'eval_samples_per_second': 14.009, 'eval_steps_per_second': 8.005, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                              \n",
      " 27%|â–ˆâ–ˆâ–‹       | 8/30 [00:18<00:43,  1.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.16672100126743317, 'eval_runtime': 0.4836, 'eval_samples_per_second': 14.474, 'eval_steps_per_second': 8.271, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 10/30 [00:23<00:48,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 3.5244, 'grad_norm': 0.17256700992584229, 'learning_rate': 0.00021999999999999995, 'epoch': 2.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 12/30 [00:27<00:35,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.15623600780963898, 'eval_runtime': 0.4859, 'eval_samples_per_second': 14.405, 'eval_steps_per_second': 8.231, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 16/30 [00:37<00:27,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.1481381207704544, 'eval_runtime': 0.4894, 'eval_samples_per_second': 14.304, 'eval_steps_per_second': 8.174, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:45<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1466, 'grad_norm': 0.1711512953042984, 'learning_rate': 0.00011999999999999999, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 20/30 [00:46<00:19,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.14263422787189484, 'eval_runtime': 0.4889, 'eval_samples_per_second': 14.319, 'eval_steps_per_second': 8.182, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 24/30 [00:55<00:11,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13836123049259186, 'eval_runtime': 0.4841, 'eval_samples_per_second': 14.459, 'eval_steps_per_second': 8.262, 'epoch': 6.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      " 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 28/30 [01:04<00:03,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13543245196342468, 'eval_runtime': 0.4853, 'eval_samples_per_second': 14.425, 'eval_steps_per_second': 8.243, 'epoch': 7.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 0.1324, 'grad_norm': 0.13623031973838806, 'learning_rate': 1.9999999999999998e-05, 'epoch': 7.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                               \n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 0.13461095094680786, 'eval_runtime': 0.49, 'eval_samples_per_second': 14.286, 'eval_steps_per_second': 8.164, 'epoch': 7.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [01:10<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 71.0028, 'train_samples_per_second': 7.605, 'train_steps_per_second': 0.423, 'train_loss': 1.2677945613861084, 'epoch': 7.59}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=30, training_loss=1.2677945613861084, metrics={'train_runtime': 71.0028, 'train_samples_per_second': 7.605, 'train_steps_per_second': 0.423, 'total_flos': 1227843132456960.0, 'train_loss': 1.2677945613861084, 'epoch': 7.592592592592593})"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00,  6.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity: 0.13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {eval_results['eval_loss']:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./llama_finetuned_resume\\\\tokenizer_config.json',\n",
       " './llama_finetuned_resume\\\\special_tokens_map.json',\n",
       " './llama_finetuned_resume\\\\tokenizer.json')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"./llama_finetuned_resume\")\n",
    "tokenizer.save_pretrained(\"./llama_finetuned_resume\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
