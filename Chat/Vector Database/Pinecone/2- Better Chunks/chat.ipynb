{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anees\\Desktop\\Coding\\llmrepo\\train-LLAMA\\.venv\\Lib\\site-packages\\pinecone\\data\\index.py:1: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from pinecone import Pinecone\n",
    "import os\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from dotenv import load_dotenv\n",
    "from collections import defaultdict\n",
    "from IPython.display import display, clear_output\n",
    "from textwrap import wrap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc = Pinecone(os.getenv(\"PINECONE_KEY\"))\n",
    "index = pc.Index(\"resume-index-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = OpenAI(\n",
    "    base_url='http://localhost:11434/v1',\n",
    "    api_key='ollama'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_chatbot(user_messsage, conversation_history):\n",
    "    try:\n",
    "        conversation_history.append(user_messsage)\n",
    "        response_stream = client.chat.completions.create(\n",
    "            model=\"mistral:latest\",\n",
    "            messages=conversation_history,\n",
    "            stream=True\n",
    "        )\n",
    "\n",
    "        output_box = \"\" \n",
    "        for chunk in response_stream:\n",
    "            if chunk.choices and len(chunk.choices) > 0:\n",
    "                delta = chunk.choices[0].delta\n",
    "                if delta and delta.content:\n",
    "                    output_box += delta.content\n",
    "                    display(output_box)  \n",
    "                    clear_output(wait=True) \n",
    "\n",
    "        conversation_history.append({\"role\": \"assistant\", \"content\": output_box})\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_to_chat(query):\n",
    "    # Encode query and retrieve relevant chunks as before\n",
    "    embedded_query = model.encode(query).tolist()\n",
    "    retrieved_chunks = index.query(\n",
    "        vector=embedded_query,\n",
    "        top_k=5,\n",
    "        include_metadata=True\n",
    "    )\n",
    "\n",
    "    dic = defaultdict(list)\n",
    "    for match in retrieved_chunks.matches:\n",
    "        section = match.metadata.get('section', 'Unknown Section')\n",
    "        text = match.metadata.get('text', '')\n",
    "        dic[section].append(text)\n",
    "\n",
    "\n",
    "    chunks_text = \"\"\n",
    "    for section, texts in dic.items():\n",
    "        chunks_text += f\"{section}:\\n\"\n",
    "        for text in texts:\n",
    "            chunks_text += f\"{text}\\n\\n\"\n",
    "\n",
    "    prompt_header = (\n",
    "        \"You are Chotu the resume assistant chatbot for Aneesh Patne. \"\n",
    "        \"You will be asked a question based on the resume data. \"\n",
    "        \"You must provide a detailed, relevant, and professional response strictly based on the resume information. \"\n",
    "        \"you shall not mention from where you got the information. \"\n",
    "        \"If the question cannot be answered using the resume data, state that you can only answer questions related to the resume.\\n\\n\"\n",
    "        \"You are a resume assistant chatbot for Aneesh Patne. Using ONLY the following information from the resume, answer the question below:\\n\\n\"\n",
    "    )\n",
    "    prompt_footer = (\n",
    "        f\"Question: \\\"{query}\\\"\\n\\n\"\n",
    "        \"Provide a detailed, relevant, and professional response strictly based on the resume information above. \"\n",
    "        \"If the question cannot be answered using the resume data, state that you can only answer questions related to the resume.\"\n",
    "        \"You will in no way provide help which is not related to the resume\"\n",
    "        \"You are free to make any assumptions based on the resume data. \"\n",
    "        \"If the questions is about the skills, you have to keep a positive tone and say yes but it would be better if you contact Aneesh Patne for more information. \"\n",
    "        \"if the question is open-ended based on the resume data, you are free to answer it in your own words. and tell the user to contact Aneesh Patne for more information. \"\n",
    "        \"you will politely state that you can only respond based on the resume.\"\n",
    "    )\n",
    "    prompt = prompt_header + chunks_text + prompt_footer\n",
    "\n",
    "    user_message = {\"role\": \"user\", \"content\": prompt}\n",
    "\n",
    "    \n",
    "    conversation_history = [{\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            \"You are a professional Resume Assistant bot for Aneesh Patne. \"\n",
    "            \"Your entire purpose is to answer questions strictly based on Aneesh Patne's resume data. \"\n",
    "            \"Do not generate answers, instructions, or advice that are not directly supported by the resume information provided. \"\n",
    "            \"If a question falls outside the scope of the resume, politely state that you can only respond based on the resume.\"\n",
    "        )\n",
    "    }]\n",
    "\n",
    "    LLM_chatbot(user_message, conversation_history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" Yes, I am a Resume Assistant specifically for Aneesh Patne. From his resume, it is clear that he is an experienced Electronics and Telecommunication professional with expertise in areas such as machine learning, web development, cloud platforms, and more. His recent project involved developing a chatbot utilizing Retrieval-Augmented Generation (RAG) for accurate and context-specific responses during March 2023 to June 2023. Additionally, he has been certified as a Machine Learning Specialist by Coursera. I would be pleased to help you learn more about Aneesh Patne's experience, skills, or expertise; however, for a more detailed discussion or conversation, it is best to reach out directly to him through his provided contact information:\\n\\nLinkedIn - [https://www.linkedin.com/in/aneeshpatne](https://www.linkedin.com/in/aneeshpatne)\\nE-mail - aneeshpatne12@gmail.com\\nGitHub  - [https://github.com/aneeshpatne](https://github.com/aneeshpatne)\\nLeetCode  - [https://leetcode.com/aneeshpatne](https://leetcode.com/aneeshpatne)\\n\\nI can answer questions related to his skills, projects, or certifications based on the resume data provided here.\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Are you resume assistant for Aneesh Patne?\"\n",
    "query_to_chat(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
